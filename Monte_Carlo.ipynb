{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chutes and Ladders using Monte Carlo Learning\n",
        "This notebook uses Monte Carlo to solve the Chutes and Ladders modified game.\n",
        "\n",
        "The game board is shown below.  Players start on Square 0 (outside the board) and move towards the goal space (State 100).  Landing at the bottom of ladder moves the player to the top, while landing at the top of a chute moves the player to the bottom square.  \n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16k2EflsluUXCPVWVgL-vyv5-IzOqWZvW\" alt=\"Drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "At each turn, the player may choose one of four dice (Effron Dice)\n",
        "- Blue: 3,3,3,3,3,3\n",
        "- Black: 4,4,4,4,0,0\n",
        "- Red: 6,6,2,2,2,2\n",
        "- Green: 5,5,5,1,1,1\n",
        "\n",
        "The **purpose** of this code is to determine which dice to select at each turn so as to minimize the number of steps it takes to reach the goal state.  "
      ],
      "metadata": {
        "id": "-BaijKVAhZer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUmkwtFLrCw_"
      },
      "outputs": [],
      "source": [
        "# Import Necessary Libraries\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from statistics import mean\n",
        "from sklearn import svm\n",
        "from matplotlib import cm\n",
        "from scipy import linalg as la\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.decomposition import PCA\n",
        "from datetime import datetime\n",
        "from packaging import version\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State Transition Code"
      ],
      "metadata": {
        "id": "BbcI9bIrhigR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nextState(state,roll):\n",
        "    '''\n",
        "    This function transitions from the current state and current dice roll to the next state.\n",
        "    INPUTS:\n",
        "        state is the current state you are in (0 to 100)\n",
        "        roll is the number showing on the dice (1 to 6)\n",
        "    RETURN VALUE:\n",
        "    this function returns the next state integer\n",
        "    '''\n",
        "    # we create a dictionary for the ladders and chutes.  The key is the start state of the chute/ladder\n",
        "    # and the value is the ending state.\n",
        "    ladders = {1:38,4:14,9:31,21:42,28:84,36:44,51:67,71:91,80:100}\n",
        "    chutes = {16:6,48:26,49:11,56:53,62:19,64:60,87:24,93:73,95:75,98:78}\n",
        "\n",
        "    next_state = state + roll\n",
        "    if next_state > 100:\n",
        "        next_state = 100\n",
        "    # now check for ladders\n",
        "    if next_state in ladders:\n",
        "        next_state = ladders[next_state]\n",
        "    # now check for chutes\n",
        "    if next_state in chutes:\n",
        "        next_state = chutes[next_state]\n",
        "\n",
        "    return next_state\n",
        "\n",
        "def roll(dice_color):\n",
        "    '''\n",
        "    This function randomly rolls one of the four effron dice.\n",
        "    INPUT:\n",
        "    dice_color should be among \"red\",\"blue\",\"black\", or \"green\"\n",
        "    OUTPUT:\n",
        "    an integer randomly selected from one of the dice\n",
        "\n",
        "    Note:\n",
        "    red = 0\n",
        "    blue = 1\n",
        "    black = 2\n",
        "    green = 3\n",
        "    '''\n",
        "\n",
        "    if dice_color == 0:\n",
        "        return random.choice([2,2,2,2,6,6])\n",
        "    if dice_color == 1:\n",
        "        return 3\n",
        "    if dice_color == 2:\n",
        "        return random.choice([0,0,4,4,4,4])\n",
        "    if dice_color == 3:\n",
        "        return random.choice([1,1,1,5,5,5])\n",
        "    # for invalid input\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "hLL4nGnJroxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo Learning Approach\n",
        "\n",
        "Below, we impliment a Monte Carlo Learning model that playes our modified version of Chutes and Ladders. Monte Carlo learning is a relatively basic approach to Reinforcement Learning. Essentially, Monte Carlo learning is done by taking many different samples, or in this case trajectories, and averaging the rewards seen for each state. To be sure, hundreds, if not thousands, of samples, must be taken in order to get a realistic estimate. In this case, thousands of trajectories are taken through the Chutes and Ladders board.\n",
        "\n",
        "To train our Monte Carlo, we have to use an assortment of algorithms. Remember, there are two \"sections\" of the learning process that take place:\n",
        "\n",
        "1. Policy Evaluation: Includes playing the game and updating our Q matrix. Importantly, we freeze the policy, meaning we play the game and do not update our policy to allow the learning process to continue.\n",
        "\n",
        "2. Policy Improvement: Includes updating our policy using our Q matrix, which is updated in the policy evaluation step.\n",
        "\n",
        "\n",
        "Within each code chunk is a short description of the algorithm."
      ],
      "metadata": {
        "id": "Jxnt4TFMjXLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------\n",
        "def update (s,a):\n",
        "  '''\n",
        "  Compute next state given state s and action a\n",
        "  s = number of dollars in pot\n",
        "  return reward\n",
        "  '''\n",
        "\n",
        "  chosenRoll = roll(a)\n",
        "  nextS = nextState(s, chosenRoll)\n",
        "\n",
        "  return nextS, 1\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def e_greedy(s,policy,epsilon):\n",
        "  '''\n",
        "  Implements an e-greedy policy.\n",
        "  With probability epsilon, it returns random action choice\n",
        "  otherwise returns action choice specified by the policy\n",
        "\n",
        "  s = current state\n",
        "  policy = policy function (an array that is indexed by state)\n",
        "  epsilon (0 to 1) a probability of picking exploratory random action\n",
        "  '''\n",
        "  r = np.random.random()\n",
        "  if r > epsilon:\n",
        "    return policy[s]\n",
        "  else:\n",
        "    return np.random.randint(0,4)\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def make_trajectory(policy,epsilon):\n",
        "  '''\n",
        "  Simulate one trajectory of experience\n",
        "  Return list of tuples during trajectory\n",
        "  Each tuple is (s,a,r) -> state / action / reward\n",
        "  epsilon = probability of exploratory action\n",
        "  '''\n",
        "  traj = []\n",
        "  s=0\n",
        "\n",
        "  while (s < 100):\n",
        "    a = e_greedy(s,policy,epsilon)\n",
        "    s_prev = s\n",
        "    s,r = update(s,a)\n",
        "    #print(s)\n",
        "    traj.append((s_prev,a,r))\n",
        "\n",
        "    if s < 0:\n",
        "      s = 0\n",
        "\n",
        "  # final reward = state value, final action = 0 (meaningless)\n",
        "  traj.append((s,0,0))\n",
        "  return traj\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def init():\n",
        "  '''\n",
        "  Create totals, counts and policy defaults\n",
        "  '''\n",
        "  totals = np.zeros((101,4), dtype=int)\n",
        "  counts = np.zeros((101,4),dtype=int)\n",
        "  P = np.zeros(101, dtype=int)\n",
        "  return totals,counts,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_improvement(Q):\n",
        "  '''\n",
        "  Update value function V and policy P based on Q values\n",
        "  '''\n",
        "  V = np.min(Q,axis=1)\n",
        "  P = np.argmin(Q,axis=1)\n",
        "  return V,P\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_evaluation(totals,counts,policy,n,epsilon):\n",
        "  '''\n",
        "  do n trajectories of learning\n",
        "  and update the v/count arrays\n",
        "  '''\n",
        "  for i in range(n):\n",
        "    t = make_trajectory(policy,epsilon)\n",
        "    m = len(t)\n",
        "    sum_r = np.zeros(m)\n",
        "    sum_r[m-1] = t[-1][2]\n",
        "    for j in range(m-2,-1,-1):\n",
        "\n",
        "      sum_r[j] = sum_r[j+1] + t[j][2]\n",
        "\n",
        "    for j in range(m):\n",
        "      s,a,r = t[j]\n",
        "      if s == 100:\n",
        "        s = 99\n",
        "      counts[s,a] += 1\n",
        "      totals[s,a] += sum_r[j]\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def policy_iteration(totals,counts,policy,Q,n,m,epsilon):\n",
        "  '''\n",
        "  Perform n iterations of policy iteration\n",
        "  using m trials (episodes) per policy update\n",
        "  '''\n",
        "  for i in range(n):\n",
        "    Q = compute_Q(totals,counts)\n",
        "    V,P = policy_improvement(Q)\n",
        "    policy_evaluation(totals,counts,P,m,epsilon)\n",
        "\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------\n",
        "def compute_Q(totals,counts):\n",
        "  '''\n",
        "  Compute the Q values based on totals and counts (average)\n",
        "  '''\n",
        "  Q = np.zeros((101,4))\n",
        "  for i in range(len(totals)):\n",
        "    for a in range(4):\n",
        "      if counts[i][a] > 0:\n",
        "\n",
        "        Q[i][a] = round((totals[i][a] / counts[i][a]), 2)\n",
        "\n",
        "      else:\n",
        "        Q[i][a] = 0\n",
        "  return Q"
      ],
      "metadata": {
        "id": "KfFpy3fyr6oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will print a few sample trajectories as well as the average move for the few trajectories that we have seen.\n",
        "\n",
        "Note: this algorithm does involve randomness. When you rune this algorithm you may see different results."
      ],
      "metadata": {
        "id": "2eWHsvYZlG8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "totals,counts,P = init() #initialize values\n",
        "epsilon = 0.1 # 10% change of a random move being taken\n",
        "sumt = 0\n",
        "k = 10\n",
        "for i in range(k):\n",
        "  t = make_trajectory(P,epsilon)\n",
        "  sumt += t[-1][-1]\n",
        "  if k <= 10:\n",
        "    print(t)\n",
        "print(\"Average Action:\",sumt/k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz8rrSaH46LT",
        "outputId": "6b49eb26-dce9-4cc9-e2d3-208e5aa98d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 0, 1), (2, 0, 1), (14, 0, 1), (6, 0, 1), (12, 0, 1), (14, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (12, 0, 1), (18, 0, 1), (20, 2, 1), (24, 0, 1), (26, 0, 1), (32, 0, 1), (34, 0, 1), (44, 0, 1), (46, 0, 1), (26, 0, 1), (84, 0, 1), (86, 0, 1), (92, 0, 1), (78, 0, 1), (84, 2, 1), (88, 0, 1), (90, 0, 1), (92, 0, 1), (78, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (6, 0, 1), (8, 0, 1), (10, 1, 1), (13, 0, 1), (15, 0, 1), (42, 0, 1), (44, 0, 1), (50, 0, 1), (53, 0, 1), (55, 0, 1), (57, 0, 1), (63, 0, 1), (65, 0, 1), (67, 0, 1), (69, 0, 1), (75, 0, 1), (81, 1, 1), (84, 0, 1), (86, 0, 1), (88, 1, 1), (91, 0, 1), (97, 0, 1), (99, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (2, 0, 1), (8, 0, 1), (10, 0, 1), (12, 0, 1), (14, 0, 1), (20, 2, 1), (20, 0, 1), (22, 0, 1), (84, 0, 1), (86, 0, 1), (92, 0, 1), (78, 0, 1), (84, 0, 1), (86, 0, 1), (88, 0, 1), (90, 0, 1), (96, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (12, 0, 1), (14, 0, 1), (20, 0, 1), (22, 0, 1), (24, 0, 1), (26, 0, 1), (32, 0, 1), (34, 0, 1), (44, 3, 1), (11, 0, 1), (13, 0, 1), (15, 0, 1), (17, 0, 1), (19, 0, 1), (42, 0, 1), (44, 0, 1), (46, 0, 1), (52, 0, 1), (54, 0, 1), (60, 0, 1), (19, 1, 1), (22, 0, 1), (84, 0, 1), (86, 0, 1), (88, 1, 1), (91, 0, 1), (97, 3, 1), (78, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (12, 2, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (6, 0, 1), (12, 0, 1), (14, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (6, 0, 1), (8, 0, 1), (14, 0, 1), (6, 0, 1), (8, 0, 1), (14, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (12, 3, 1), (13, 0, 1), (15, 0, 1), (42, 0, 1), (26, 0, 1), (32, 3, 1), (37, 0, 1), (39, 0, 1), (41, 0, 1), (43, 0, 1), (11, 0, 1), (13, 0, 1), (19, 0, 1), (42, 0, 1), (26, 0, 1), (32, 0, 1), (38, 3, 1), (43, 0, 1), (11, 0, 1), (13, 0, 1), (15, 0, 1), (17, 0, 1), (23, 0, 1), (25, 0, 1), (31, 0, 1), (33, 0, 1), (35, 0, 1), (41, 1, 1), (44, 0, 1), (50, 0, 1), (53, 0, 1), (55, 0, 1), (57, 0, 1), (59, 0, 1), (61, 0, 1), (63, 0, 1), (69, 0, 1), (91, 0, 1), (73, 0, 1), (75, 0, 1), (77, 0, 1), (83, 0, 1), (89, 0, 1), (91, 0, 1), (73, 0, 1), (79, 0, 1), (85, 0, 1), (91, 0, 1), (73, 0, 1), (79, 0, 1), (85, 0, 1), (24, 0, 1), (30, 0, 1), (32, 0, 1), (38, 0, 1), (40, 0, 1), (42, 0, 1), (44, 0, 1), (46, 0, 1), (26, 0, 1), (32, 2, 1), (44, 0, 1), (50, 0, 1), (52, 0, 1), (54, 0, 1), (53, 0, 1), (59, 0, 1), (61, 0, 1), (63, 0, 1), (65, 2, 1), (69, 0, 1), (91, 2, 1), (75, 0, 1), (77, 0, 1), (79, 0, 1), (81, 0, 1), (83, 0, 1), (89, 0, 1), (75, 0, 1), (81, 0, 1), (24, 0, 1), (30, 0, 1), (32, 0, 1), (34, 0, 1), (44, 0, 1), (46, 0, 1), (52, 0, 1), (58, 0, 1), (60, 0, 1), (66, 0, 1), (68, 0, 1), (74, 0, 1), (76, 0, 1), (82, 0, 1), (88, 0, 1), (94, 0, 1), (96, 0, 1), (78, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (2, 0, 1), (14, 0, 1), (6, 0, 1), (12, 0, 1), (18, 0, 1), (20, 0, 1), (22, 0, 1), (24, 0, 1), (26, 0, 1), (32, 0, 1), (34, 0, 1), (44, 0, 1), (50, 0, 1), (52, 1, 1), (55, 0, 1), (57, 0, 1), (63, 0, 1), (65, 0, 1), (67, 0, 1), (69, 2, 1), (73, 0, 1), (75, 0, 1), (77, 0, 1), (79, 0, 1), (81, 0, 1), (24, 0, 1), (30, 1, 1), (33, 0, 1), (35, 0, 1), (37, 0, 1), (43, 0, 1), (45, 0, 1), (47, 0, 1), (53, 0, 1), (55, 0, 1), (57, 0, 1), (59, 0, 1), (61, 0, 1), (67, 0, 1), (73, 0, 1), (75, 0, 1), (77, 0, 1), (83, 0, 1), (89, 0, 1), (91, 0, 1), (73, 0, 1), (75, 0, 1), (77, 0, 1), (83, 3, 1), (84, 0, 1), (86, 0, 1), (88, 0, 1), (90, 0, 1), (92, 0, 1), (94, 0, 1), (96, 1, 1), (99, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (2, 0, 1), (14, 0, 1), (6, 0, 1), (12, 0, 1), (14, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (12, 0, 1), (14, 0, 1), (20, 0, 1), (26, 0, 1), (32, 0, 1), (38, 0, 1), (44, 0, 1), (46, 0, 1), (52, 0, 1), (54, 0, 1), (53, 0, 1), (55, 0, 1), (57, 0, 1), (59, 0, 1), (61, 0, 1), (67, 0, 1), (69, 0, 1), (91, 0, 1), (73, 0, 1), (75, 0, 1), (77, 0, 1), (79, 1, 1), (82, 0, 1), (84, 0, 1), (90, 3, 1), (75, 0, 1), (77, 0, 1), (79, 0, 1), (81, 0, 1), (83, 0, 1), (85, 0, 1), (91, 0, 1), (97, 0, 1), (99, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (2, 0, 1), (8, 0, 1), (14, 0, 1), (20, 0, 1), (26, 0, 1), (84, 0, 1), (86, 0, 1), (88, 0, 1), (94, 0, 1), (96, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (2, 0, 1), (14, 0, 1), (6, 0, 1), (8, 0, 1), (10, 0, 1), (12, 0, 1), (18, 0, 1), (20, 0, 1), (22, 0, 1), (84, 0, 1), (86, 0, 1), (88, 0, 1), (90, 3, 1), (91, 0, 1), (73, 0, 1), (75, 0, 1), (81, 0, 1), (83, 0, 1), (85, 0, 1), (91, 0, 1), (73, 0, 1), (75, 0, 1), (77, 0, 1), (83, 0, 1), (85, 0, 1), (91, 0, 1), (73, 3, 1), (78, 0, 1), (100, 0, 0)]\n",
            "[(0, 0, 1), (2, 0, 1), (14, 0, 1), (20, 0, 1), (26, 0, 1), (32, 0, 1), (38, 0, 1), (40, 0, 1), (42, 0, 1), (44, 0, 1), (50, 0, 1), (53, 0, 1), (55, 0, 1), (61, 0, 1), (67, 3, 1), (72, 0, 1), (74, 0, 1), (76, 0, 1), (78, 0, 1), (84, 0, 1), (86, 0, 1), (88, 0, 1), (90, 0, 1), (92, 0, 1), (78, 0, 1), (84, 0, 1), (86, 0, 1), (88, 0, 1), (94, 0, 1), (96, 0, 1), (78, 0, 1), (100, 0, 0)]\n",
            "Average Action: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code trains and then tests our model as described at the start of this section. It will print the performance of our algorithm: Q-Matrix, V list, and policy after the final trial."
      ],
      "metadata": {
        "id": "0cXkKSKJmA-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(seed=42) # if you would like to repeat same process\n",
        "\n",
        "totals,counts,P = init() # initialize values\n",
        "\n",
        "# run experiment on a larger basis\n",
        "m = 10\n",
        "n = 15000\n",
        "epsilon = 0.1 # 10% change of a random move being taken\n",
        "Q = compute_Q(totals,counts)\n",
        "V,P = policy_improvement(Q)\n",
        "policy_iteration(totals,counts,P,Q,n,m,epsilon)\n",
        "\n",
        "Q = compute_Q(totals,counts)\n",
        "V,P = policy_improvement(Q)\n",
        "print('Q =\\n',Q)\n",
        "print('V =\\n',V)\n",
        "print('P =\\n',P)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiTpa_v88OMg",
        "outputId": "513a147d-eb99-46af-9d45-91fa1d14ac73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q =\n",
            " [[1.6070e+01 1.6750e+01 1.5070e+01 1.2700e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.5780e+01 1.4820e+01 1.6560e+01 1.7000e+01]\n",
            " [3.1140e+01 1.6050e+01 1.7310e+01 1.4830e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.6430e+01 1.4780e+01 1.3050e+01 1.5740e+01]\n",
            " [1.5100e+01 1.5770e+01 1.7510e+01 1.7410e+01]\n",
            " [2.2960e+01 1.9030e+01 1.6790e+01 1.4960e+01]\n",
            " [1.7370e+01 1.6610e+01 1.7530e+01 1.3500e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.9240e+01 2.3090e+01 1.8420e+01 1.4830e+01]\n",
            " [1.9230e+01 8.1610e+01 2.0140e+01 1.5330e+01]\n",
            " [1.7410e+01 1.5190e+01 1.7090e+01 1.3780e+01]\n",
            " [1.3470e+01 1.8380e+01 1.5640e+01 1.5090e+01]\n",
            " [1.5120e+01 9.8510e+01 1.9860e+01 1.7200e+01]\n",
            " [1.2080e+01 1.5900e+01 1.4650e+01 1.4260e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.5400e+01 7.3780e+01 1.3690e+01 1.2110e+01]\n",
            " [1.1940e+01 1.8410e+01 1.2360e+01 1.4620e+01]\n",
            " [1.0930e+01 3.4520e+01 1.4400e+01 1.4790e+01]\n",
            " [1.1000e+01 9.3370e+01 1.3320e+01 1.4110e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.0250e+01 3.0400e+01 1.2520e+01 1.2840e+01]\n",
            " [1.8430e+01 2.3410e+01 1.9380e+01 2.0550e+01]\n",
            " [1.1000e+01 6.4700e+01 1.1450e+01 1.6650e+01]\n",
            " [1.9520e+01 1.0161e+02 1.6240e+01 1.1460e+01]\n",
            " [8.3700e+00 2.7830e+01 1.2180e+01 1.9220e+01]\n",
            " [4.1000e+01 1.6058e+02 3.6000e+01 1.2160e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.8090e+01 1.0528e+02 1.3830e+01 2.2420e+01]\n",
            " [1.7530e+01 8.7590e+01 2.4180e+01 1.2170e+01]\n",
            " [1.2940e+01 1.3520e+01 1.1720e+01 1.0160e+01]\n",
            " [1.3550e+01 3.3850e+01 1.0080e+01 1.3060e+01]\n",
            " [6.7070e+01 1.6160e+01 1.2450e+01 2.0450e+01]\n",
            " [1.2060e+01 3.6400e+01 1.1720e+01 1.4870e+01]\n",
            " [2.3250e+01 1.1490e+01 1.1840e+01 1.0140e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.5670e+01 9.1790e+01 1.2200e+01 1.0900e+01]\n",
            " [1.0110e+01 1.1210e+01 1.0720e+01 1.0950e+01]\n",
            " [3.9980e+01 1.0150e+01 1.1070e+01 1.0350e+01]\n",
            " [9.2300e+00 1.9510e+01 1.0370e+01 9.5300e+00]\n",
            " [2.2100e+01 9.6300e+00 9.0100e+00 1.1060e+01]\n",
            " [1.0000e+01 1.1190e+01 1.2730e+01 8.9500e+00]\n",
            " [1.2640e+01 4.0870e+01 8.4000e+00 9.7600e+00]\n",
            " [8.5500e+00 7.9400e+00 1.2300e+01 1.2420e+01]\n",
            " [7.2600e+00 1.4710e+01 1.3490e+01 8.9700e+00]\n",
            " [1.0900e+01 2.9300e+01 9.6700e+00 6.9200e+00]\n",
            " [1.4360e+01 1.0620e+01 6.5200e+00 1.0650e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.1370e+01 1.7780e+01 1.0330e+01 7.9100e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [1.0590e+01 1.4770e+01 1.1520e+01 1.0200e+01]\n",
            " [2.2990e+01 1.2651e+02 1.0000e+01 2.3260e+01]\n",
            " [9.9200e+00 4.4290e+01 1.3230e+01 1.0370e+01]\n",
            " [8.9000e+00 1.2520e+01 9.9900e+00 9.4400e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [8.0700e+00 1.1900e+01 8.4900e+00 1.2230e+01]\n",
            " [7.0940e+01 1.4240e+01 1.5070e+01 7.8500e+00]\n",
            " [7.3800e+00 2.1210e+01 1.0310e+01 8.4300e+00]\n",
            " [5.5800e+01 7.1500e+00 1.1910e+01 1.8870e+01]\n",
            " [6.7500e+00 8.7200e+00 6.8100e+00 1.0330e+01]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [6.1500e+00 6.0600e+00 7.5400e+00 7.3200e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [5.3300e+00 8.1200e+00 5.9100e+00 5.9500e+00]\n",
            " [5.8900e+00 5.7900e+00 5.9100e+00 5.0500e+00]\n",
            " [6.2600e+00 5.5200e+00 4.7900e+00 5.5300e+00]\n",
            " [4.7900e+00 1.2080e+01 7.1200e+00 7.3900e+00]\n",
            " [4.8000e+00 5.4100e+00 6.9800e+00 4.5500e+00]\n",
            " [5.7900e+00 8.4800e+00 4.2600e+00 4.8300e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [3.8800e+00 6.7600e+00 6.8500e+00 5.1600e+00]\n",
            " [6.7800e+00 2.1200e+01 4.7410e+01 5.2300e+00]\n",
            " [5.2500e+00 2.4500e+00 5.2200e+00 6.9800e+00]\n",
            " [6.3300e+00 4.3600e+00 1.6460e+01 5.2600e+00]\n",
            " [2.0530e+01 8.9770e+01 3.1750e+01 6.1400e+00]\n",
            " [9.4100e+00 1.0000e+00 2.6920e+01 6.2400e+00]\n",
            " [2.9700e+00 1.4090e+01 7.2100e+00 8.2800e+00]\n",
            " [8.9900e+00 4.2860e+01 1.9390e+01 5.1600e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [8.8400e+00 1.8320e+01 6.8480e+01 1.0070e+01]\n",
            " [8.6170e+01 2.5790e+01 7.8500e+00 3.0480e+01]\n",
            " [7.2700e+00 7.0000e+00 1.0690e+01 6.1800e+00]\n",
            " [5.8500e+00 3.9330e+01 6.8500e+00 7.5500e+00]\n",
            " [2.5110e+01 3.0580e+01 8.1290e+01 5.5800e+00]\n",
            " [5.0600e+00 8.4900e+00 5.5300e+00 8.3200e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [5.3500e+00 4.2600e+00 1.0390e+01 6.9700e+00]\n",
            " [5.1800e+00 2.4300e+01 7.8470e+01 1.9410e+01]\n",
            " [3.7300e+00 1.2880e+01 1.1110e+01 4.9400e+00]\n",
            " [5.7900e+00 3.1400e+00 5.1800e+00 3.3700e+00]\n",
            " [4.3400e+00 1.0210e+01 3.1400e+00 4.9600e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [2.0600e+00 2.1100e+00 4.3900e+00 3.6200e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [3.3200e+00 2.0100e+00 1.5300e+00 1.5400e+00]\n",
            " [1.6600e+00 1.0000e+00 1.2900e+00 3.3200e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
            " [3.0000e-02 1.0000e+00 1.2900e+00 1.0000e+00]\n",
            " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
            "V =\n",
            " [12.7   0.   14.82 14.83  0.   13.05 15.1  14.96 13.5   0.   14.83 15.33\n",
            " 13.78 13.47 15.12 12.08  0.   12.11 11.94 10.93 11.    0.   10.25 18.43\n",
            " 11.   11.46  8.37 12.16  0.   13.83 12.17 10.16 10.08 12.45 11.72 10.14\n",
            "  0.   10.9  10.11 10.15  9.23  9.01  8.95  8.4   7.94  7.26  6.92  6.52\n",
            "  0.    0.    7.91  0.   10.2  10.    9.92  8.9   0.    8.07  7.85  7.38\n",
            "  7.15  6.75  0.    6.06  0.    5.33  5.05  4.79  4.79  4.55  4.26  0.\n",
            "  3.88  5.23  2.45  4.36  6.14  1.    2.97  5.16  0.    8.84  7.85  6.18\n",
            "  5.85  5.58  5.06  0.    4.26  5.18  3.73  3.14  3.14  0.    2.06  0.\n",
            "  1.53  1.    0.    0.03  0.  ]\n",
            "P =\n",
            " [3 0 1 3 0 2 0 3 3 0 3 3 3 0 0 0 0 3 0 0 0 0 0 0 0 3 0 3 0 2 3 3 2 2 2 3 0\n",
            " 3 0 1 0 2 3 2 1 0 3 2 0 0 3 0 3 2 0 0 0 0 3 0 1 0 0 1 0 0 3 2 0 3 2 0 0 3\n",
            " 1 1 3 1 0 3 0 0 2 3 0 3 0 0 1 0 0 1 2 0 0 0 2 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `assess` function will conduct a series of non-learning trials and evaluate the value of the policy. In this case, that value would be the average number of moves it takes to complete the game across all trials."
      ],
      "metadata": {
        "id": "BBlAM_Nqm4Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------\n",
        "def assess(policy,trials):\n",
        "  '''\n",
        "  Assess the value of the current policy by completing #trials\n",
        "  using the specified policy (no e-greedy random actions)\n",
        "  Does not accrue learning experience nor change policy\n",
        "  '''\n",
        "\n",
        "  policy_evaluation(totals,counts,policy,trials,0)\n",
        "  Q = compute_Q(totals,counts)\n",
        "  V,P = policy_improvement(Q)\n",
        "  return V[0]"
      ],
      "metadata": {
        "id": "EPb0iJLKm55L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#value = assess(P,2000)\n",
        "print(\"Average moves per game: \" + str(assess(P,2000)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m308TguJTX-0",
        "outputId": "ce53ed88-c0bd-4446-c5c1-50f7ff370bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average moves per game: 12.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Before we analyze the results, it is essential to remember that each time this notebook is run, it is likely there will be a slightly different result.\n",
        "\n",
        "\n",
        "The above policy takes an average of 12.05 moves to complete a game of Chutes and Ladders, which is relatively close to the optimal value of 8 moves. The Monte Carlo approach discovered some interesting moves. For example, on the first move, our policy tells us to roll the green die. This makes sense because if we land on the 1st state, we hit a ladder and are taken all the way to 38. There are some peculiar decisions as well. For example, on square 92, it tells us that we should roll the green die. This is interesting because the green die has $\\frac{1}{2}$ chance of rolling a one, which would hit a chute and take us all the way to 72. But there is a $\\frac{1}{2}$ chance of rolling a five, which would lead us to finish the game in 2 moves because after a five is rolled, we would land on state 97. From there, our policy would tell us to roll the blue die, which would take us to the end. That is an interesting choice to make, given the risk.\n",
        "\n",
        "\n",
        "Although the number of moves is higher than some of the other models, the average number of moves needed to reach the end is by no means absurdly high. Monte Carlo remains an important topic in Reinforcement Learning because it is a useful approach to take on its own and is crucial in the implementation of the Q-Learning and SARSA algorithms discussed below."
      ],
      "metadata": {
        "id": "RRjkEfoXogTo"
      }
    }
  ]
}