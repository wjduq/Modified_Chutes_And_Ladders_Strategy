{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D06HLrgr_3pB"
      },
      "source": [
        "# Chutes and Ladders using Q-Learning\n",
        "This notebook uses Q-Learning to solve the Chutes and Ladders modified game.\n",
        "\n",
        "The game board is shown below.  Players start on Square 0 (outside the board) and move towards the goal space (State 100).  Landing at the bottom of ladder moves the player to the top, while landing at the top of a chute moves the player to the bottom square.  \n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16k2EflsluUXCPVWVgL-vyv5-IzOqWZvW\" alt=\"Drawing\" width=\"500\"/>\n",
        "\n",
        "\n",
        "At each turn, the player may choose one of four dice (Effron Dice)\n",
        "- Blue: 3,3,3,3,3,3\n",
        "- Black: 4,4,4,4,0,0\n",
        "- Red: 6,6,2,2,2,2\n",
        "- Green: 5,5,5,1,1,1\n",
        "\n",
        "The **purpose** of this code is to determine which dice to select at each turn so as to minimize the number of steps it takes to reach the goal state.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liPxRhku_3pD"
      },
      "outputs": [],
      "source": [
        "# import statements\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9d3-37f_3pE"
      },
      "source": [
        "## State Transition Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1ow_Tdi_3pE"
      },
      "outputs": [],
      "source": [
        "def nextState (state,roll):\n",
        "    '''\n",
        "    This function transitions from the current state and current dice roll to the next state.\n",
        "    INPUTS:\n",
        "        state is the current state you are in (0 to 100)\n",
        "        roll is the number showing on the dice (1 to 6)\n",
        "    RETURN VALUE:\n",
        "    this function returns the next state integer\n",
        "    '''\n",
        "    # we create a dictionary for the ladders and chutes.  The key is the start state of the chute/ladder\n",
        "    # and the value is the ending state.\n",
        "    ladders = {1:38,4:14,9:31,21:42,28:84,36:44,51:67,71:91,80:100}\n",
        "    chutes = {16:6,48:26,49:11,56:53,62:19,64:60,87:24,93:73,95:75,98:78}\n",
        "\n",
        "    next_state = state + roll\n",
        "    if next_state > 100:\n",
        "        next_state = 100\n",
        "    # now check for ladders\n",
        "    if next_state in ladders:\n",
        "        next_state = ladders[next_state]\n",
        "    # now check for chutes\n",
        "    if next_state in chutes:\n",
        "        next_state = chutes[next_state]\n",
        "\n",
        "    return next_state\n",
        "\n",
        "def roll (dice_color):\n",
        "    '''\n",
        "    This function randomly rolls one of the four effron dice.\n",
        "    INPUT:\n",
        "    dice_color should be among \"red\",\"blue\",\"black\", or \"green\"\n",
        "    OUTPUT:\n",
        "    an integer randomly selected from one of the dice\n",
        "    '''\n",
        "\n",
        "    if dice_color == 'red':\n",
        "        return random.choice([2,2,2,2,6,6])\n",
        "    if dice_color == 'blue':\n",
        "        return 3\n",
        "    if dice_color == 'black':\n",
        "        return random.choice([0,0,4,4,4,4])\n",
        "    if dice_color == 'green':\n",
        "        return random.choice([1,1,1,5,5,5])\n",
        "    # for invalid input\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning Approach"
      ],
      "metadata": {
        "id": "39apShXKUm9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we will implement a Q-Learning model that plays our modified version of Chutes and Ladders. Above each cell is a brief description of what the code in the cell does. Generally, we will be training our model across ten 1,000 game trials, updating our policy at the conclusion of each. Afterward, we will test our model by playing 2,000 games using only our policy in order to get an accurate measure of the average number of moves it takes to complete a game."
      ],
      "metadata": {
        "id": "XmGdnoqYJtPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `egreedy` function returns the action our model will take on each turn, which will either be the move our policy suggests or a random move, depending on the result of an RNG. There is a 10% chance of a random move being taken."
      ],
      "metadata": {
        "id": "r-cuVfEUT1z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def egreedy(epsilon, s, policy, debug):\n",
        "    a = 0\n",
        "    epsChoice = (random.random() <= epsilon) # If true, will take a random action instead of best action\n",
        "    if epsChoice: # random action\n",
        "        a = random.randint(0,3)\n",
        "        if debug:\n",
        "            print(\"Performing a random action, a =\", a)\n",
        "    else: # pick best action based on Q\n",
        "        a = policy[s]\n",
        "        if debug:\n",
        "            print(\"Performing best known action, a =\", a)\n",
        "    return a"
      ],
      "metadata": {
        "id": "4CgM_xCaT2BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `updatePolicy` function updates our policy. It is run before each training epoch. This function updates our policy by finding the minimum value move at each state using the model's Q-matrix."
      ],
      "metadata": {
        "id": "zqmMEFKjVUxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def updatePolicy(Q):\n",
        "    \"\"\"\n",
        "    Updates the model's policy based on Q\n",
        "    Inputs: Q = Q-matrix\n",
        "    Outputs: new policy\n",
        "    \"\"\"\n",
        "\n",
        "    V = np.min(Q, axis=1)\n",
        "    P = np.argmin(Q, axis=1)\n",
        "    return V,P"
      ],
      "metadata": {
        "id": "4hYJ8cPwVpTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``Q-Learning`` method plays through one game of modified Chutes and Ladders, adjusting the Q-matrix after each move. There is a random chance (10%) of the model making a random action instead of the best known action according to the Q-matrix in order to encourage exploration. The model learns at a rate of 0.1, giving the result of each move a slight influence over the value of each state.\n",
        "\n",
        "This function will only ever be run from inside of the ``QLTrain`` function."
      ],
      "metadata": {
        "id": "6pFcAOOJKBaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QLearning(Q, policy, aOpts, alpha, epsilon, debug):\n",
        "    \"\"\"\n",
        "    Plays 1 game of Chutes and Ladders to train the model\n",
        "    Inputs: Q = Q-matrix\n",
        "            policy = the current policy of the model\n",
        "            aOpts = a list of action choices\n",
        "            alpha = alpha value\n",
        "            epsilon = epsilon value\n",
        "            debug = True to print out debug messages\n",
        "    Outputs: Q at end of game\n",
        "    \"\"\"\n",
        "\n",
        "    s = 0 # state, start @ square 0\n",
        "    total_reward = 0\n",
        "    reward = 0\n",
        "    mov_count = 0\n",
        "    while s != 100:\n",
        "        if debug:\n",
        "            print(\"Move\", str(mov_count) + \":\")\n",
        "        a = egreedy(epsilon, s, policy, debug)\n",
        "\n",
        "        # Get next state\n",
        "        rollVal = roll(aOpts[a])\n",
        "        sP = nextState(s, rollVal) # s'\n",
        "        reward = 1\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        # Debug stuff\n",
        "        if debug:\n",
        "            print(\"Result: Rolled\", aOpts[a], \"die with result\", rollVal, \"moving from square\", s, \"to square\", (s + rollVal))\n",
        "            if (s + rollVal) == sP:\n",
        "                print(\"No chutes or ladders encountered, final square = \" + str(sP) + \", reward =\", reward)\n",
        "            elif (s + rollVal) < sP:\n",
        "                print(\"Ladder encountered, final square = \" + str(sP) + \", reward =\", reward)\n",
        "            else:\n",
        "                print(\"Chute encountered, final square = \" + str(sP) + \", reward =\", reward)\n",
        "        prevQ = Q[s][a]\n",
        "        prevS = s\n",
        "\n",
        "        # Q update calculations\n",
        "        aP = policy[sP] # a'\n",
        "        TDerror = reward + Q[sP][aP] - Q[s][a]\n",
        "        Q[s][a] = Q[s][a] + alpha*TDerror\n",
        "        s = sP\n",
        "\n",
        "        if debug:\n",
        "            print(\"Previous Q-value at state\", prevS, \"choosing the\", aOpts[a], \"die =\", prevQ)\n",
        "            print(\"New Q-value at state\", prevS, \"choosing the\", aOpts[a], \"die =\", Q[s][a])\n",
        "            print()\n",
        "        mov_count += 1\n",
        "\n",
        "    # update terminal state (unnecessary?)\n",
        "    reward = 0 # I guess\n",
        "    total_reward += reward\n",
        "    TDerror = reward + 0 - Q[s][a]\n",
        "    Q[s][a] = Q[s][a] + alpha * TDerror\n",
        "\n",
        "    return total_reward"
      ],
      "metadata": {
        "id": "5EyKblPoUpnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``QLTrain`` function is used to train the Q-Learning model. To do this it plays ``n`` games of modified Chutes and Ladders."
      ],
      "metadata": {
        "id": "O9nxbScCKgCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QLTrain(Q, policy, aOpts, n, alpha, epsilon, debug=False):\n",
        "    \"\"\"\n",
        "    Trains a model using QLearning\n",
        "    Inputs: Q = Q-Matrix\n",
        "            policy = policy of the model\n",
        "            aOpts = a list of action choices\n",
        "            numEpochs = number of training epochs\n",
        "            alpha = alpha value\n",
        "            epsilon = epsilon value\n",
        "            debug = True to print debug messages (False by default)\n",
        "    Outputs: Q at end of training\n",
        "    \"\"\"\n",
        "\n",
        "    R = 0   # total reward\n",
        "    for i in range(n):\n",
        "        R += QLearning(Q, policy, aOpts, alpha, epsilon, debug)\n",
        "    return R / n"
      ],
      "metadata": {
        "id": "P3pymvpMPjR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``QLPlay`` function will play a game of modified Chutes and Ladders based only on its Q-matrix. This is run after training the model to illustrate results. ``PrintGame`` prints a formatted log of the game."
      ],
      "metadata": {
        "id": "4UOuQLaBK8Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QLPlay(aOpts, policy, debug=False, printLog=False):\n",
        "    \"\"\"\n",
        "    Plays a game of Modified Chutes and Ladders without updating Q-matrix.\n",
        "    Prints a log of actions (if debug is True) and the path taken through the game.\n",
        "    Inputs: aOpts = a list of action choices\n",
        "            policy = Q-matrix\n",
        "            debug = True to print debug messages\n",
        "    Outputs: Returns number of moves and a log of actions and states, prints a log of the game to the console.\n",
        "    \"\"\"\n",
        "\n",
        "    if debug:\n",
        "        print(\"Game Debug Log:\")\n",
        "    sLog = []\n",
        "    aLog = []\n",
        "    s = 0\n",
        "    mov_count = 0\n",
        "    while s != 100:\n",
        "        a = policy[s]\n",
        "        sLog.append(s)\n",
        "        if debug:\n",
        "            print(\"Move \" + str(mov_count) + \":\")\n",
        "        aLog.append(a)\n",
        "        rollVal = roll(aOpts[a])\n",
        "        sP = nextState(s, rollVal) # s'\n",
        "\n",
        "        # Debug stuff\n",
        "        if debug:\n",
        "            print(\"Selected the\", aOpts[a], \"die (Q-value = \" + str(Q[s][a]) + \"). Rolled a \" + str(rollVal) + \".\")\n",
        "            if (s + rollVal) == sP:\n",
        "                print(\"Moving from square\", s, \"to square \" + str(s + rollVal) + \".\")\n",
        "            elif (s + rollVal) < sP:\n",
        "                print(\"There was a ladder on square \" + str((s+rollVal)) + \"! It went to square \" + str(sP) + \"!\")\n",
        "            else:\n",
        "                print(\"There was a chute on square \" + str((s + rollVal)) + \"! It went to square \" + str(sP) + \"!\")\n",
        "            if (s < sP):\n",
        "                print(\"This action resulted in a net movement of\", (sP - s), \"squares forward.\")\n",
        "            elif (s == sP):\n",
        "                print(\"This action resulted in no movement.\")\n",
        "            else:\n",
        "                print(\"This action resulted in a net movement of\", (sP - s), \"squares backward.\")\n",
        "\n",
        "        s = sP\n",
        "        mov_count += 1\n",
        "\n",
        "    if debug:\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    if printLog:\n",
        "        PrintGame(aOpts, sLog, aLog) # Game Result Output\n",
        "    return(mov_count)"
      ],
      "metadata": {
        "id": "ImH0TVBsK7og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def PrintGame(aOpts, sLog, aLog):\n",
        "    \"\"\"\n",
        "    Prints a log of a game described by sLog and aLog\n",
        "    Inputs: aOpts = a list of action choices\n",
        "            sLog = log of all states in order\n",
        "            aLog = log of action choices in order\n",
        "    Outputs: Returns nothing, prints to console\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Log:\")\n",
        "    for i in range(len(sLog)):\n",
        "        if i == len(sLog)-1:\n",
        "            print(\"Move \" + str(i + 1) + \": Chose \" + aOpts[aLog[i]] + \" die, square \" + str(sLog[i]) + \" -> square 100\")\n",
        "        else:\n",
        "            print(\"Move \" + str(i + 1) + \": Chose \" + aOpts[aLog[i]] + \" die, square \" + str(sLog[i]) + \" -> square \" + str(sLog[i+1]))"
      ],
      "metadata": {
        "id": "oaS_n0mcOuvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code trains and then tests our model as described at the start of this section. It will print the performance (the number of average number of moves to finish the game) for each trial of training and the final Q-matrix, list of values (V list), and policy after the final trial."
      ],
      "metadata": {
        "id": "Q0g7Y_kRK87N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = 10\n",
        "n = 1000\n",
        "\n",
        "aOpts = [\"red\", \"blue\", \"black\", \"green\"]\n",
        "policy = np.zeros(101, dtype=int)\n",
        "Q = np.full((101,4), 0.0, dtype=float) # Initialize to 1\n",
        "epsilon = 0.1\n",
        "alpha = 0.1\n",
        "\n",
        "for i in range(m):\n",
        "    print(\"\\n*** Trial {0:d} ***\".format(i))\n",
        "    R = QLTrain(Q, policy, aOpts, n, alpha, epsilon, debug=False)\n",
        "    V,policy = updatePolicy(Q)\n",
        "    print(\"Performance:\", R)\n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"FINAL Q MATRIX\")\n",
        "print(\"=============================\")\n",
        "print(\"Note: Rows of 0s in the matrix are a result of those squares being the beginning of a chute or ladder path.\")\n",
        "print()\n",
        "print(Q)\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"FINAL V LIST\")\n",
        "print(\"=============================\")\n",
        "print(\"Shows the value of the optimal action at each square.\")\n",
        "print()\n",
        "print(V)\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"FINAL POLICY\")\n",
        "print(\"=============================\")\n",
        "print(\"Each item is the die to select at the corresponding square (starting from 0 up to 100.\")\n",
        "print(\"Dice key: 0 = Red, 1 = Blue, 2 = Black, 3 = Green\")\n",
        "print()\n",
        "print(policy)\n",
        "print()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKq96d2MP5Oe",
        "outputId": "225498a1-1082-4b26-97f9-924c2ae840d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** Trial 0 ***\n",
            "Performance: 29.675\n",
            "\n",
            "*** Trial 1 ***\n",
            "Performance: 21.535\n",
            "\n",
            "*** Trial 2 ***\n",
            "Performance: 63.863\n",
            "\n",
            "*** Trial 3 ***\n",
            "Performance: 12.494\n",
            "\n",
            "*** Trial 4 ***\n",
            "Performance: 12.858\n",
            "\n",
            "*** Trial 5 ***\n",
            "Performance: 12.479\n",
            "\n",
            "*** Trial 6 ***\n",
            "Performance: 11.972\n",
            "\n",
            "*** Trial 7 ***\n",
            "Performance: 12.191\n",
            "\n",
            "*** Trial 8 ***\n",
            "Performance: 12.122\n",
            "\n",
            "*** Trial 9 ***\n",
            "Performance: 12.104\n",
            "\n",
            "\n",
            "FINAL Q MATRIX\n",
            "=============================\n",
            "Note: Rows of 0s in the matrix are a result of those squares being the beginning of a chute or ladder path.\n",
            "\n",
            "[[ 12.99832244  15.75437403  10.99959319  10.38577398]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 11.70915444  67.86501892  90.17178692 130.69128513]\n",
            " [ 97.04464436  31.30097125  14.82858201  43.08457474]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 13.02212805  12.95276793   9.98521397  13.50169473]\n",
            " [ 12.39788008  61.27333024  89.53651846  85.99924716]\n",
            " [ 75.50555649  13.2750994   81.8679767   61.24875124]\n",
            " [ 18.16586695  11.85195546  15.17342498  77.39890758]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 12.21999889  74.47835112  32.95049937  31.62143884]\n",
            " [ 38.35741134  10.92205424  26.54795498  30.49680125]\n",
            " [ 10.58563166  16.25823741  15.75326721  68.32518191]\n",
            " [ 42.22982961 192.06953816 101.96288925  12.20198751]\n",
            " [ 11.03646594  10.08091575   9.90494457   9.97548841]\n",
            " [ 10.91589599   9.43486303  10.46742685  24.95950578]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  8.92414695   9.42975296  13.78744198  17.83499318]\n",
            " [  8.48378062   9.95518712   8.89334201   8.84248823]\n",
            " [  9.42504262   9.16205599   9.21684913   8.10994488]\n",
            " [  8.30875807   8.28194722   8.09648576   7.97992753]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  6.96450525 151.45207712  61.60016826  24.22213913]\n",
            " [  8.47567974  13.16194883   9.95772257   7.01367147]\n",
            " [ 10.79474624  10.87432426   6.35842144   9.18780988]\n",
            " [ 80.79324141   6.23626789  77.84639271  47.97478647]\n",
            " [  6.97916399 138.02462753 124.49824528 143.8484358 ]\n",
            " [ 38.93161512 159.33575906   9.8973029   18.49045446]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 90.97142839   9.2041951   86.50491398 184.24088504]\n",
            " [ 10.50670669 187.71509    181.51396089 177.29709076]\n",
            " [ 17.56741764  13.55832192  10.95118617   8.50778816]\n",
            " [ 15.83616481  20.7442799    8.1047955   22.17343856]\n",
            " [162.96128584   8.97385595 181.54302761 195.10472974]\n",
            " [  8.79792367 150.88664421 141.33740324 136.42946782]\n",
            " [153.56599781 153.31808776 162.68396135   8.4537179 ]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [152.71211924 162.03027822 168.9086448   11.53839987]\n",
            " [  8.45062759   9.03549229   9.30600469  10.59990109]\n",
            " [152.51224943   9.800549   133.07501317 192.66250702]\n",
            " [  8.15599661 102.00543273 127.35507484 146.2645174 ]\n",
            " [ 65.42026116 107.79155122 132.12314427   7.45009314]\n",
            " [  7.93148285   7.31759639   7.35313978   7.32729238]\n",
            " [ 71.99103382   6.96676748  24.73197103 116.78857402]\n",
            " [  9.52558549   6.96711918  10.67064558  10.47261748]\n",
            " [  6.34881106 161.82487213 161.09797192  45.01942509]\n",
            " [  9.2603341   12.23111817  10.12846492   6.13110727]\n",
            " [ 11.58969197  11.45314993   5.85912868   9.41400437]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 10.48896355  98.77702859  15.14146269  16.57866426]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 15.72564083   9.19801025  11.52590944  17.07223935]\n",
            " [ 24.75516224  86.26748332  12.98788815   9.88666777]\n",
            " [ 15.07527619   8.25187363  23.8262033    9.11857595]\n",
            " [ 15.40076741  40.16904706   8.40731142  50.81325224]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 13.41676731  14.05709461   7.40487321  17.42051097]\n",
            " [ 21.88454979  17.58618398  10.28041124  26.44602592]\n",
            " [ 11.07264803   8.34460825   7.17825439  90.1522514 ]\n",
            " [ 10.96551702   6.53053247 138.29383765  34.8238332 ]\n",
            " [ 11.22422692  76.18906328   6.42294672   7.45008195]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 12.34877395   5.666559     5.84912436   6.3108379 ]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 16.99960933   4.72436763   4.88957535   4.97428359]\n",
            " [  4.45860758   4.73163146   4.75726198   4.97333343]\n",
            " [  4.31236484   4.38856018   4.26066516   4.30573921]\n",
            " [  3.64687723   3.98134157   4.16405172   4.11334299]\n",
            " [  8.05722785   3.78425202   5.59699521   3.60194742]\n",
            " [  3.35468739   3.51234856   3.52475954   3.31818082]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  3.19703053   2.73013471   2.80156769   2.87634454]\n",
            " [  4.15772984   2.44135979   2.78553743   3.17272527]\n",
            " [  2.45651559   2.           3.53608049   3.65680539]\n",
            " [  4.302663     3.56238454   3.62194936   1.7415437 ]\n",
            " [  5.53894195   4.48355367   1.67775673   4.63908084]\n",
            " [  6.19284716   1.           5.40708018   3.73583871]\n",
            " [  2.73419362   7.49977944   5.20756672   7.83224677]\n",
            " [ 11.08785541   5.94501137   6.64266976   4.10216912]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [ 12.7885064    6.34081724   8.10297969   6.18721535]\n",
            " [  6.58564596   6.05923663   6.12812046   6.52988757]\n",
            " [ 15.64918199   5.45574328   8.16160327   5.52648902]\n",
            " [  5.1560701    7.66265626   5.34950557   5.11463528]\n",
            " [  6.94595025   4.86865681   5.11692327   5.03011149]\n",
            " [  4.55668305   4.77185647   4.93122995   5.16983571]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  4.26384399   4.13538762   4.22092229   4.20355542]\n",
            " [  3.92288315   3.82235933   3.85122864   3.80428355]\n",
            " [  3.55403626   3.48879903   3.39291822   3.75082132]\n",
            " [  4.1699895    3.00131119   3.54866392   3.04294558]\n",
            " [  3.06247542   2.74560949   3.04166071   2.85932003]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  2.11347636   2.00003316   3.77586065   2.31695178]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  2.81146993   1.99353057   1.59359121   1.32564233]\n",
            " [  1.76083386   1.           1.30654255   2.24380324]\n",
            " [  0.           0.           0.           0.        ]\n",
            " [  0.99999788   0.99937342   1.58931806   0.99820299]\n",
            " [  0.           0.           0.           0.        ]]\n",
            "\n",
            "\n",
            "FINAL V LIST\n",
            "=============================\n",
            "Shows the value of the optimal action at each square.\n",
            "\n",
            "[10.38577398  0.         11.70915444 14.82858201  0.          9.98521397\n",
            " 12.39788008 13.2750994  11.85195546  0.         12.21999889 10.92205424\n",
            " 10.58563166 12.20198751  9.90494457  9.43486303  0.          8.92414695\n",
            "  8.48378062  8.10994488  7.97992753  0.          6.96450525  7.01367147\n",
            "  6.35842144  6.23626789  6.97916399  9.8973029   0.          9.2041951\n",
            " 10.50670669  8.50778816  8.1047955   8.97385595  8.79792367  8.4537179\n",
            "  0.         11.53839987  8.45062759  9.800549    8.15599661  7.45009314\n",
            "  7.31759639  6.96676748  6.96711918  6.34881106  6.13110727  5.85912868\n",
            "  0.          0.         10.48896355  0.          9.19801025  9.88666777\n",
            "  8.25187363  8.40731142  0.          7.40487321 10.28041124  7.17825439\n",
            "  6.53053247  6.42294672  0.          5.666559    0.          4.72436763\n",
            "  4.45860758  4.26066516  3.64687723  3.60194742  3.31818082  0.\n",
            "  2.73013471  2.44135979  2.          1.7415437   1.67775673  1.\n",
            "  2.73419362  4.10216912  0.          6.18721535  6.05923663  5.45574328\n",
            "  5.11463528  4.86865681  4.55668305  0.          4.13538762  3.80428355\n",
            "  3.39291822  3.00131119  2.74560949  0.          2.00003316  0.\n",
            "  1.32564233  1.          0.          0.99820299  0.        ]\n",
            "\n",
            "\n",
            "FINAL POLICY\n",
            "=============================\n",
            "Each item is the die to select at the corresponding square (starting from 0 up to 100.\n",
            "Dice key: 0 = Red, 1 = Blue, 2 = Black, 3 = Green\n",
            "\n",
            "[3 0 0 2 0 2 0 1 1 0 0 1 0 3 2 1 0 0 0 3 3 0 0 3 2 1 0 2 0 1 0 3 2 1 0 3 0\n",
            " 3 0 1 0 3 1 1 1 0 3 2 0 0 0 0 1 3 1 2 0 2 2 2 1 2 0 1 0 1 0 2 0 3 3 0 1 1\n",
            " 1 3 2 1 0 3 0 3 1 1 3 1 0 0 1 3 2 1 1 0 1 0 3 1 0 3 0]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `assess` function will conduct a series of non-learning trials and evaluate the value of the policy. In this case, that value would be the average number of moves it takes to complete the game across all trials."
      ],
      "metadata": {
        "id": "nr3q5lGHdAAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assess(policy, aOpts, numTrials):\n",
        "    \"\"\"\n",
        "    Runs numTrials non-learning trials and evaluates the value of the policy.\n",
        "    Inputs: policy = policy of the model\n",
        "            numTrials = number of non-learning trials to run\n",
        "    Outputs: value of the policy (avg. moves to win)\n",
        "    \"\"\"\n",
        "    totalMovesAllTrials = 0\n",
        "    for i in range(numTrials):\n",
        "        totalMovesAllTrials += QLPlay(aOpts, policy, debug=False, printLog=False)\n",
        "    return float(totalMovesAllTrials)/float(numTrials)"
      ],
      "metadata": {
        "id": "yqpnXTjPdOHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will use the `assess` function to evaluate the value of our policy after training."
      ],
      "metadata": {
        "id": "Xy-RdjWDeNZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average moves per game:\", assess(policy, aOpts, 2000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xgVidCeeVln",
        "outputId": "e5e453c9-a4dd-41c4-e914-c6d05116dd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average moves per game: 10.453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This measure of average moves can vary some, but generally gives results of approximately 10.5 moves per game on average. These results are quite promising, as the average game of Chutes and Ladders takes 39.2 turns (using the same values as we have on these dice), with the best possible game taking 7 turns. Accounting for bad rolls, this is quite a good result."
      ],
      "metadata": {
        "id": "LC8b21dpJyNm"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}